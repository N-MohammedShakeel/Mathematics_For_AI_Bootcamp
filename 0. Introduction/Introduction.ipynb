{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mathematics for AI/ML Engineers: Roadmap\n",
        "\n",
        "This notebook provides a high-level roadmap for the mathematical concepts essential for AI/ML engineers. For each topic, we list subtopics and explain why they are necessary for AI/ML, with minimal code snippets to show practical relevance.\n",
        "\n",
        "**Purpose**: Use this as a guide to plan your learning. Focus on understanding the role of each topic in AI/ML and refer to detailed resources (e.g., 'Mathematics for Machine Learning' by Deisenroth, Khan Academy, or 3Blue1Brown) for deeper study.\n",
        "\n",
        "**How to Use**: Review each section, run the code snippets to see applications, and prioritize topics based on your AI/ML goals (e.g., beginner projects vs. research).\n",
        "\n",
        "**Prerequisites**: Basic Python, high-school math (algebra, trigonometry)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Linear Algebra\n",
        "\n",
        "**Why Necessary**: Linear algebra is the foundation for representing and manipulating data in AI/ML. Vectors and matrices are used for datasets, model parameters (e.g., neural network weights), and transformations (e.g., rotations in computer vision). It enables efficient computation for algorithms like PCA and neural networks.\n",
        "\n",
        "**Subtopics**:\n",
        "- Vectors and vector spaces\n",
        "- Matrices (operations, inverses, determinants, eigenvalues/eigenvectors)\n",
        "- Linear transformations and systems of equations\n",
        "- Dimensionality reduction (PCA, SVD)\n",
        "- Orthogonality and least squares\n",
        "\n",
        "**High-Level Code Example**: Matrix multiplication for data transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix multiplication (e.g., transforming data points)\n",
        "A = np.array([[2, 1], [1, 3]])  # Transformation matrix\n",
        "x = np.array([1, 2])  # Data point\n",
        "result = np.dot(A, x)\n",
        "print(f'Transformed point: {result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Calculus\n",
        "\n",
        "**Why Necessary**: Calculus is critical for optimization in AI/ML, especially for training models via gradient descent. Derivatives measure how loss functions change, guiding updates to model parameters. Multivariable calculus is used in neural networks for backpropagation.\n",
        "\n",
        "**Subtopics**:\n",
        "- Limits, continuity, and derivatives\n",
        "- Integrals (definite/indefinite)\n",
        "- Multivariable calculus (gradients, Hessians, Jacobians)\n",
        "- Series and approximations (e.g., Taylor series)\n",
        "- Optimization basics (minima, constrained optimization)\n",
        "\n",
        "**High-Level Code Example**: Gradient descent for optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple gradient descent for f(x) = x^2\n",
        "x = 4.0\n",
        "learning_rate = 0.1\n",
        "for _ in range(5):\n",
        "    x -= learning_rate * 2*x  # Derivative of x^2\n",
        "print(f'Optimized x: {x}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Probability and Statistics\n",
        "\n",
        "**Why Necessary**: Probability models uncertainty in data and predictions (e.g., classification probabilities). Statistics helps analyze data, evaluate models (e.g., via p-values), and understand distributions for generative models or Bayesian methods.\n",
        "\n",
        "**Subtopics**:\n",
        "- Probability basics (events, conditional probability, Bayes' theorem)\n",
        "- Random variables (discrete/continuous, expectations, variance)\n",
        "- Distributions (normal, binomial, Poisson, exponential)\n",
        "- Statistical inference (hypothesis testing, confidence intervals)\n",
        "- Regression and correlation\n",
        "\n",
        "**High-Level Code Example**: Visualizing a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot normal distribution\n",
        "data = np.random.normal(0, 1, 1000)\n",
        "plt.hist(data, bins=30, density=True)\n",
        "plt.title('Normal Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Discrete Mathematics\n",
        "\n",
        "**Why Necessary**: Discrete math supports algorithms and structures like graphs, which are used in AI applications (e.g., recommendation systems, graph neural networks). It’s less critical for beginners but useful for advanced algorithm design.\n",
        "\n",
        "**Subtopics**:\n",
        "- Sets, functions, and relations\n",
        "- Graph theory (nodes, edges, paths, trees)\n",
        "- Combinatorics (permutations, combinations)\n",
        "- Logic and proofs (propositional logic, induction)\n",
        "- Recursion and algorithms basics\n",
        "\n",
        "**High-Level Code Example**: Basic graph representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adjacency matrix for a simple graph\n",
        "graph = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 0]])  # 3 nodes\n",
        "print(f'Graph adjacency matrix:\\n{graph}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Optimization\n",
        "\n",
        "**Why Necessary**: Optimization techniques minimize loss functions to train accurate models. Understanding methods like gradient descent or constrained optimization is key for tuning neural networks and other algorithms.\n",
        "\n",
        "**Subtopics**:\n",
        "- Convex optimization (convex functions, duality)\n",
        "- Gradient-based methods (GD, SGD, Adam)\n",
        "- Constrained optimization (Lagrange multipliers)\n",
        "- Stochastic and non-convex optimization\n",
        "- Linear/quadratic programming\n",
        "\n",
        "**High-Level Code Example**: Stochastic gradient descent (simplified)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified SGD\n",
        "weights = np.array([1.0])\n",
        "for _ in range(5):\n",
        "    gradient = 0.5 * weights  # Mock gradient\n",
        "    weights -= 0.01 * gradient\n",
        "print(f'Updated weights: {weights}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Plan\n",
        "\n",
        "- **Sequence**: Start with Linear Algebra and Calculus (2-4 weeks each), then Probability/Statistics (3-5 weeks), Discrete Math if needed (1-2 weeks), and Optimization last (2-3 weeks).\n",
        "- **Study Approach**: Focus on intuition and application. Use visuals (e.g., plots) and code to understand concepts. Derive key formulas once or twice, then apply in projects.\n",
        "- **Resources**: Khan Academy (basics), 3Blue1Brown (visuals), Coursera (Andrew Ng’s ML), Python (NumPy, Matplotlib) for practice.\n",
        "- **Practice**: Apply to toy datasets or simple ML models (e.g., linear regression). Experiment with code snippets above."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
